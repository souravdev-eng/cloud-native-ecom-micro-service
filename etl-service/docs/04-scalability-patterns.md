# ğŸ“š Chapter 4: Scalability Patterns for ETL

## ğŸ¯ Learning Objectives

By the end of this chapter, you will understand:

- How to scale ETL processes for millions of records
- Horizontal vs vertical scaling strategies
- Partitioning and parallelization techniques
- Memory management for large datasets
- Real-world scaling patterns used by major companies

---

## ğŸ“ˆ The Scalability Challenge

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCALING CHALLENGES                                â”‚
â”‚                                                                      â”‚
â”‚   Today:        1,000 products    â†’ ETL runs in 5 seconds           â”‚
â”‚   Next month:   100,000 products  â†’ ETL runs in 8 minutes           â”‚
â”‚   Next year:    10,000,000 products â†’ ??? (Hours? Days?)            â”‚
â”‚                                                                      â”‚
â”‚   As data grows, simple approaches break down!                      â”‚
â”‚                                                                      â”‚
â”‚   Problems at scale:                                                â”‚
â”‚   â”œâ”€â”€ Memory exhaustion (can't load all data)                      â”‚
â”‚   â”œâ”€â”€ Timeout errors (queries too slow)                            â”‚
â”‚   â”œâ”€â”€ Network bottlenecks (too much data transfer)                 â”‚
â”‚   â”œâ”€â”€ Database locks (blocking other operations)                   â”‚
â”‚   â””â”€â”€ Single point of failure (one error kills everything)         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Pattern 1: Batch Processing (What We Use)

### Current Implementation

```typescript
// Process in chunks instead of all at once
for (let i = 0; i < missingProducts.length; i += batchSize) {
  const batch = missingProducts.slice(i, i + batchSize);
  await productRepository.save(batch);
  await this.delay(100);
}
```

### Why It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               BATCH PROCESSING BENEFITS                              â”‚
â”‚                                                                      â”‚
â”‚   MEMORY USAGE                                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚   All at once:  Load 1M products = 500MB RAM at once               â”‚
â”‚   In batches:   Load 100 products = 50KB RAM at a time             â”‚
â”‚                                                                      â”‚
â”‚   100 products â”€â”€â”                                                  â”‚
â”‚                  â”œâ”€â”€â–¶ Process â”€â”€â–¶ Free memory â”€â”€â”                  â”‚
â”‚   100 products â”€â”€â”˜                              â”‚                  â”‚
â”‚                  â”œâ”€â”€â–¶ Process â”€â”€â–¶ Free memory â”€â”€â”¤                  â”‚
â”‚   100 products â”€â”€â”˜                              â”‚                  â”‚
â”‚                  ...continues...                 â”‚                  â”‚
â”‚                                                  â–¼                  â”‚
â”‚                                          Memory stays low!          â”‚
â”‚                                                                      â”‚
â”‚   DATABASE LOAD                                                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚   All at once:  INSERT 1M rows = Database locked for minutes       â”‚
â”‚   In batches:   INSERT 100 rows = Quick inserts, others can query  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Choosing the Right Batch Size

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 BATCH SIZE SELECTION                                 â”‚
â”‚                                                                      â”‚
â”‚   Size    â”‚ Pros                    â”‚ Cons                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚   10      â”‚ Low memory, quick fail  â”‚ Too many DB calls           â”‚
â”‚   50      â”‚ Fast recovery           â”‚ Still inefficient           â”‚
â”‚   100 âœ…  â”‚ Good balance            â”‚ Works for most cases        â”‚
â”‚   500     â”‚ Fewer DB calls          â”‚ Longer per-batch time       â”‚
â”‚   1000    â”‚ Very efficient          â”‚ Memory concerns             â”‚
â”‚   10000   â”‚ Maximum efficiency      â”‚ Timeout/memory issues       â”‚
â”‚                                                                      â”‚
â”‚   FORMULA for optimal batch size:                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚
â”‚   Consider:                                                         â”‚
â”‚   â€¢ Row size (small rows â†’ larger batches OK)                      â”‚
â”‚   â€¢ Transaction timeout (large batches must complete in time)      â”‚
â”‚   â€¢ Memory available (don't exceed heap limit)                     â”‚
â”‚   â€¢ Concurrent users (smaller batches = less DB blocking)          â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Pattern 2: Streaming (For Very Large Datasets)

### When to Use

When data is too large to fit in memory, even in batches.

### Concept

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STREAMING vs BATCHING                             â”‚
â”‚                                                                      â”‚
â”‚   BATCHING (Our current approach)                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚   1. Fetch ALL products from MongoDB into memory                   â”‚
â”‚   2. Then process in batches                                        â”‚
â”‚                                                                      â”‚
â”‚   Problem: With 10M products, step 1 loads 5GB into memory!        â”‚
â”‚                                                                      â”‚
â”‚   STREAMING (For massive datasets)                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚   1. Fetch products one page at a time using cursor                â”‚
â”‚   2. Process each page before fetching next                        â”‚
â”‚                                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚   â”‚ Page 1   â”‚â”€â”€â”€â–¶â”‚ Process  â”‚â”€â”€â”€â–¶â”‚ Page 2   â”‚â”€â”€â”€â–¶...              â”‚
â”‚   â”‚(100 items)â”‚   â”‚ & Write  â”‚    â”‚(100 items)â”‚                     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                      â”‚
â”‚   Memory usage stays constant regardless of total data size!        â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Example

```typescript
// Streaming approach for massive datasets
static async syncProductsStreaming(): Promise<SyncResult> {
    const batchSize = 100;
    let processed = 0;
    let synced = 0;

    // Use MongoDB cursor - doesn't load everything at once
    const cursor = MongoProduct.find({}).cursor();

    let batch: ProductDoc[] = [];

    for await (const product of cursor) {
        batch.push(product);

        if (batch.length >= batchSize) {
            // Process this batch
            const targetIds = await this.getExistingIds(batch.map(p => p.id));
            const missing = batch.filter(p => !targetIds.has(p.id));

            if (missing.length > 0) {
                await this.insertBatch(missing);
                synced += missing.length;
            }

            processed += batch.length;
            batch = [];  // Clear batch, free memory

            // Progress update
            console.log(`Processed: ${processed}, Synced: ${synced}`);
        }
    }

    // Don't forget the last partial batch!
    if (batch.length > 0) {
        // ... process remaining items
    }

    return { processed, synced };
}
```

---

## ğŸ”„ Pattern 3: Parallel Processing

### Current: Sequential Processing

```typescript
// Processes one batch at a time
for (let i = 0; i < batches.length; i++) {
  await processBatch(batches[i]); // Wait for each batch
}
// Total time: batch1 + batch2 + batch3 + ...
```

### Improved: Parallel Processing

```typescript
// Process multiple batches simultaneously
const PARALLEL_WORKERS = 4;

async function processInParallel(items: any[], batchSize: number) {
  const batches = chunkArray(items, batchSize);

  // Process 4 batches at a time
  for (let i = 0; i < batches.length; i += PARALLEL_WORKERS) {
    const currentBatches = batches.slice(i, i + PARALLEL_WORKERS);

    // All 4 run at the same time!
    await Promise.all(currentBatches.map((batch) => processBatch(batch)));
  }
}
// Total time: (batch1 || batch2 || batch3 || batch4) + ...
// Roughly 4x faster!
```

### Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 SEQUENTIAL vs PARALLEL                               â”‚
â”‚                                                                      â”‚
â”‚   SEQUENTIAL (One at a time)                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚   Time: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶     â”‚
â”‚   Worker 1: [Batch 1][Batch 2][Batch 3][Batch 4][Batch 5]...       â”‚
â”‚   Worker 2: (idle)                                                  â”‚
â”‚   Worker 3: (idle)                                                  â”‚
â”‚   Worker 4: (idle)                                                  â”‚
â”‚                                                                      â”‚
â”‚   PARALLEL (4 workers)                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚   Time: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶                             â”‚
â”‚   Worker 1: [Batch 1][Batch 5][Batch 9]...                         â”‚
â”‚   Worker 2: [Batch 2][Batch 6][Batch 10]...                        â”‚
â”‚   Worker 3: [Batch 3][Batch 7][Batch 11]...                        â”‚
â”‚   Worker 4: [Batch 4][Batch 8][Batch 12]...                        â”‚
â”‚                                                                      â”‚
â”‚   âš¡ ~4x faster total execution time!                               â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Caution: Database Connection Limits

```typescript
// Don't spawn too many parallel operations!
const MAX_PARALLEL = Math.min(
  parseInt(process.env.PARALLEL_WORKERS || '4'),
  10 // Never more than 10 to avoid connection exhaustion
);
```

---

## ğŸ”„ Pattern 4: Partitioning (Sharding)

### Concept

Split data by a key to process subsets independently.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PARTITIONING                                 â”‚
â”‚                                                                      â”‚
â”‚   ALL DATA (1 million products)                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Products A-Z (all mixed together)                          â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                       â”‚
â”‚                              â–¼                                       â”‚
â”‚   PARTITIONED BY CATEGORY                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚  Phones  â”‚  â”‚ Laptops  â”‚  â”‚ Fashion  â”‚  â”‚  Books   â”‚           â”‚
â”‚   â”‚ (50,000) â”‚  â”‚ (30,000) â”‚  â”‚(200,000) â”‚  â”‚(100,000) â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚        â–¼              â–¼              â–¼              â–¼               â”‚
â”‚   [ETL Worker 1] [ETL Worker 2] [ETL Worker 3] [ETL Worker 4]      â”‚
â”‚                                                                      â”‚
â”‚   Benefits:                                                         â”‚
â”‚   âœ… Each partition is smaller (faster to process)                 â”‚
â”‚   âœ… Partitions can run in parallel                                â”‚
â”‚   âœ… Failure in one partition doesn't affect others                â”‚
â”‚   âœ… Can prioritize important partitions                           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Example

```typescript
// Partition by seller ID
static async syncProductsBySeller(): Promise<void> {
    // Get unique sellers
    const sellers = await MongoProduct.distinct('sellerId');

    // Process each seller's products independently
    for (const sellerId of sellers) {
        console.log(`Syncing products for seller: ${sellerId}`);

        await this.syncProductsForSeller(sellerId);
    }
}

static async syncProductsForSeller(sellerId: string): Promise<SyncResult> {
    // Only fetch this seller's products
    const sourceProducts = await MongoProduct.find({ sellerId });
    const targetProducts = await CartProductRepo.find({ where: { sellerId } });

    // Rest of sync logic...
}
```

### Partition Strategies

| Strategy        | Use When                | Example                            |
| --------------- | ----------------------- | ---------------------------------- |
| **By ID Range** | IDs are sequential      | Products 1-10000, 10001-20000, ... |
| **By Hash**     | Need even distribution  | `productId.hashCode() % 4`         |
| **By Category** | Natural grouping exists | Phones, Electronics, Fashion       |
| **By Date**     | Time-series data        | Today, This Week, This Month       |
| **By Region**   | Geographic distribution | US, EU, APAC                       |

---

## ğŸ”„ Pattern 5: Checkpointing

### Problem

What if ETL crashes halfway through?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE CRASH PROBLEM                                 â”‚
â”‚                                                                      â”‚
â”‚   Without checkpointing:                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚   [Batch 1] âœ“ [Batch 2] âœ“ [Batch 3] âœ“ [Batch 4] ğŸ’¥ CRASH           â”‚
â”‚                                                                      â”‚
â”‚   Restart: Start from Batch 1 again!                               â”‚
â”‚   - Re-processes 3 batches (wasted work)                           â”‚
â”‚   - May create duplicates                                          â”‚
â”‚                                                                      â”‚
â”‚   With checkpointing:                                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚   [Batch 1] âœ“ [Save: completed=1]                                  â”‚
â”‚   [Batch 2] âœ“ [Save: completed=2]                                  â”‚
â”‚   [Batch 3] âœ“ [Save: completed=3]                                  â”‚
â”‚   [Batch 4] ğŸ’¥ CRASH                                               â”‚
â”‚                                                                      â”‚
â”‚   Restart: Read checkpoint (completed=3), start from Batch 4!      â”‚
â”‚   âœ… No wasted work                                                 â”‚
â”‚   âœ… No duplicates                                                  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Example

```typescript
interface Checkpoint {
  lastProcessedId: string;
  processedCount: number;
  startedAt: Date;
  updatedAt: Date;
}

class CheckpointedSync {
  static async syncWithCheckpoint(): Promise<void> {
    // Load last checkpoint
    let checkpoint = await this.loadCheckpoint();

    // Resume from where we left off
    const query = checkpoint ? { _id: { $gt: checkpoint.lastProcessedId } } : {};

    const cursor = MongoProduct.find(query).sort({ _id: 1 }).cursor();

    let batch: ProductDoc[] = [];
    let lastId = checkpoint?.lastProcessedId || '';

    for await (const product of cursor) {
      batch.push(product);
      lastId = product.id;

      if (batch.length >= 100) {
        await this.processBatch(batch);

        // Save checkpoint after each batch
        await this.saveCheckpoint({
          lastProcessedId: lastId,
          processedCount: (checkpoint?.processedCount || 0) + batch.length,
          startedAt: checkpoint?.startedAt || new Date(),
          updatedAt: new Date(),
        });

        batch = [];
      }
    }

    // Clear checkpoint when complete
    await this.clearCheckpoint();
  }
}
```

---

## ğŸ”„ Pattern 6: Dead Letter Queue (DLQ)

### Concept

Failed items go to a special queue for retry/investigation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEAD LETTER QUEUE                                 â”‚
â”‚                                                                      â”‚
â”‚   Normal Flow                                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
â”‚   [Product 1] â”€â”€â–¶ Transform â”€â”€â–¶ Load â”€â”€â–¶ âœ… Success                â”‚
â”‚   [Product 2] â”€â”€â–¶ Transform â”€â”€â–¶ Load â”€â”€â–¶ âœ… Success                â”‚
â”‚   [Product 3] â”€â”€â–¶ Transform â”€â”€â–¶ Load â”€â”€â–¶ âŒ Failed!                â”‚
â”‚                                              â”‚                      â”‚
â”‚                                              â–¼                      â”‚
â”‚   Dead Letter Queue                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  { product: Product3, error: "Duplicate key", time: t } â”‚      â”‚
â”‚   â”‚  { product: Product7, error: "Timeout", time: t }       â”‚      â”‚
â”‚   â”‚  { product: Product9, error: "Invalid data", time: t }  â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                              â”‚                                      â”‚
â”‚                              â–¼                                      â”‚
â”‚   Later: Retry or investigate failed items                         â”‚
â”‚                                                                      â”‚
â”‚   Benefits:                                                         â”‚
â”‚   âœ… Main process continues (doesn't block on errors)              â”‚
â”‚   âœ… Failed items preserved for analysis                           â”‚
â”‚   âœ… Can retry failures separately                                 â”‚
â”‚   âœ… Track error patterns over time                                â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Scaling Strategy Cheatsheet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               WHEN TO USE WHICH PATTERN                              â”‚
â”‚                                                                      â”‚
â”‚   Data Size        â”‚ Recommended Patterns                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚   < 10,000         â”‚ Simple batching (current approach) âœ“           â”‚
â”‚   10K - 100K       â”‚ Parallel processing + batching                 â”‚
â”‚   100K - 1M        â”‚ Streaming + partitioning + checkpoints         â”‚
â”‚   1M - 10M         â”‚ Distributed workers + message queues           â”‚
â”‚   > 10M            â”‚ Apache Spark / Flink / dedicated ETL tools     â”‚
â”‚                                                                      â”‚
â”‚   Current Service: Optimized for ~100K records                      â”‚
â”‚   (Can scale further with additional patterns)                      â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Quick Wins for Current Service

### 1. Add Parallel Batch Processing

```typescript
// Process 4 batches simultaneously
const WORKERS = 4;
const batchChunks = chunk(batches, WORKERS);
for (const group of batchChunks) {
  await Promise.all(group.map((b) => processBatch(b)));
}
```

### 2. Add Index on IDs

```sql
-- In Cart Service PostgreSQL
CREATE INDEX idx_product_id ON product(id);

-- In MongoDB
db.products.createIndex({ _id: 1 });
```

### 3. Use UPSERT Instead of Check-Then-Insert

```typescript
// Instead of: check if exists â†’ insert if not
// Use: upsert (insert or update)
await productRepository.upsert(cartProduct, ['id']);
```

---

## ğŸ§  Key Takeaways

| Concept               | What It Does                  | When to Use                |
| --------------------- | ----------------------------- | -------------------------- |
| **Batching**          | Process in chunks             | Always (basic requirement) |
| **Streaming**         | Process without loading all   | Data > available memory    |
| **Parallel**          | Multiple simultaneous workers | CPU/IO bound work          |
| **Partitioning**      | Divide by logical key         | Need isolation/priority    |
| **Checkpointing**     | Save progress                 | Long-running jobs          |
| **Dead Letter Queue** | Preserve failures             | Production systems         |

---

## â¡ï¸ Next Chapter

[Chapter 5: Distributed Systems Fundamentals](./05-distributed-systems.md)
