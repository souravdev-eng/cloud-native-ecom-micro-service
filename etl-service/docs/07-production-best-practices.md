# ğŸ“š Chapter 7: Production Best Practices

## ğŸ¯ Learning Objectives
By the end of this chapter, you will understand:
- How to deploy ETL services safely
- Kubernetes configurations for ETL workloads
- Security considerations
- Disaster recovery strategies
- Operational runbooks

---

## ğŸš€ Deployment Strategies

### Blue-Green Deployment

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BLUE-GREEN DEPLOYMENT                             â”‚
â”‚                                                                      â”‚
â”‚   Current (Blue)                    New (Green)                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   â”‚ ETL v1.0     â”‚                 â”‚ ETL v1.1     â”‚                 â”‚
â”‚   â”‚ (Running)    â”‚                 â”‚ (Deploying)  â”‚                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚          â–²                                                          â”‚
â”‚          â”‚ Traffic                                                  â”‚
â”‚          â”‚                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚              Load Balancer / Ingress             â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                      â”‚
â”‚   Steps:                                                            â”‚
â”‚   1. Deploy Green (v1.1) alongside Blue (v1.0)                     â”‚
â”‚   2. Test Green thoroughly                                          â”‚
â”‚   3. Switch traffic from Blue to Green                             â”‚
â”‚   4. Keep Blue running for quick rollback                          â”‚
â”‚   5. Decommission Blue after verification                          â”‚
â”‚                                                                      â”‚
â”‚   âœ… Zero downtime                                                  â”‚
â”‚   âœ… Easy rollback                                                  â”‚
â”‚   âš ï¸ Requires 2x resources during transition                       â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Rolling Update (Kubernetes Default)

```yaml
# k8s/etl-depl.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: etl-depl
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0    # Never have 0 pods
      maxSurge: 1          # Can temporarily have 1 extra
  template:
    spec:
      containers:
        - name: etl
          image: souravdeveloper/ecom-etl:latest
```

---

## â˜¸ï¸ Kubernetes Configuration

### Resource Limits

```yaml
# Prevent ETL from consuming too many resources
containers:
  - name: etl
    resources:
      requests:
        memory: "256Mi"    # Guaranteed memory
        cpu: "100m"        # 0.1 CPU cores
      limits:
        memory: "512Mi"    # Max memory (OOM killed if exceeded)
        cpu: "500m"        # Max CPU (throttled if exceeded)
```

**Why Set Limits?**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               RESOURCE LIMITS IMPORTANCE                             â”‚
â”‚                                                                      â”‚
â”‚   Without limits:                                                   â”‚
â”‚   â”œâ”€â”€ ETL could consume all node memory                            â”‚
â”‚   â”œâ”€â”€ Other pods get evicted                                       â”‚
â”‚   â”œâ”€â”€ Node becomes unstable                                        â”‚
â”‚   â””â”€â”€ ETL itself might get randomly killed                         â”‚
â”‚                                                                      â”‚
â”‚   With limits:                                                      â”‚
â”‚   â”œâ”€â”€ ETL stays within bounds                                      â”‚
â”‚   â”œâ”€â”€ Node remains healthy                                         â”‚
â”‚   â”œâ”€â”€ Predictable behavior                                         â”‚
â”‚   â””â”€â”€ Kubernetes can schedule efficiently                          â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Health Probes

```yaml
containers:
  - name: etl
    livenessProbe:
      httpGet:
        path: /api/etl/live
        port: 4000
      initialDelaySeconds: 30    # Wait before first check
      periodSeconds: 10          # Check every 10 seconds
      failureThreshold: 3        # Restart after 3 failures
    
    readinessProbe:
      httpGet:
        path: /api/etl/ready
        port: 4000
      initialDelaySeconds: 10
      periodSeconds: 5
      failureThreshold: 3        # Remove from service after 3 failures
```

**Probe Differences:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 LIVENESS vs READINESS                                â”‚
â”‚                                                                      â”‚
â”‚   LIVENESS: "Is the application alive?"                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚   If fails â†’ Kubernetes RESTARTS the pod                           â”‚
â”‚                                                                      â”‚
â”‚   Use for:                                                          â”‚
â”‚   â”œâ”€â”€ Detecting deadlocks                                          â”‚
â”‚   â”œâ”€â”€ Infinite loops                                                â”‚
â”‚   â””â”€â”€ Unrecoverable states                                         â”‚
â”‚                                                                      â”‚
â”‚   Our implementation:                                               â”‚
â”‚   GET /api/etl/live â†’ Always returns 200 if app is running        â”‚
â”‚                                                                      â”‚
â”‚   READINESS: "Can this pod handle traffic?"                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚   If fails â†’ Kubernetes REMOVES from service (no traffic)          â”‚
â”‚              but does NOT restart                                   â”‚
â”‚                                                                      â”‚
â”‚   Use for:                                                          â”‚
â”‚   â”œâ”€â”€ Database connections not ready                               â”‚
â”‚   â”œâ”€â”€ Dependencies not available                                   â”‚
â”‚   â””â”€â”€ Heavy processing in progress                                 â”‚
â”‚                                                                      â”‚
â”‚   Our implementation:                                               â”‚
â”‚   GET /api/etl/ready â†’ Tests all database connections              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pod Disruption Budget

```yaml
# Prevent all ETL pods from being evicted at once
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: etl-pdb
spec:
  minAvailable: 1    # At least 1 pod must stay running
  selector:
    matchLabels:
      app: etl
```

---

## ğŸ” Security Best Practices

### 1. Secret Management

```yaml
# DON'T: Hardcode secrets
env:
  - name: MONGO_PASSWORD
    value: "my-secret-password"  # âŒ NEVER DO THIS!

# DO: Use Kubernetes Secrets
env:
  - name: MONGO_PASSWORD
    valueFrom:
      secretKeyRef:
        name: etl-secrets
        key: mongo-password    # âœ… Stored securely
```

### 2. Least Privilege Principle

```typescript
// ETL user should have MINIMAL database permissions
// Product MongoDB: READ ONLY
// Cart PostgreSQL: INSERT, UPDATE (no DELETE, no DROP)
// Order MongoDB: INSERT, UPDATE
```

### 3. Network Policies

```yaml
# Only allow ETL to talk to specific databases
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: etl-network-policy
spec:
  podSelector:
    matchLabels:
      app: etl
  policyTypes:
    - Egress
  egress:
    - to:
        - podSelector:
            matchLabels:
              app: product-mongo
      ports:
        - port: 27017
    - to:
        - podSelector:
            matchLabels:
              app: cart-postgres
      ports:
        - port: 5432
```

### 4. API Authentication

```typescript
// All sync endpoints require authentication + admin role
router.post(
    '/api/etl/sync',
    requireAuth,           // Must be logged in
    restrictTo('admin'),   // Must have admin role
    async (req, res) => { ... }
);
```

---

## ğŸ›¡ï¸ Disaster Recovery

### Scenario 1: Database Corruption

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               DATABASE CORRUPTION RECOVERY                           â”‚
â”‚                                                                      â”‚
â”‚   Problem: Cart PostgreSQL data is corrupted                        â”‚
â”‚                                                                      â”‚
â”‚   Steps:                                                            â”‚
â”‚   1. STOP ETL scheduler immediately                                â”‚
â”‚      curl -X POST /api/etl/scheduler/stop                          â”‚
â”‚                                                                      â”‚
â”‚   2. Restore PostgreSQL from backup                                 â”‚
â”‚      pg_restore -d cart backup.dump                                â”‚
â”‚                                                                      â”‚
â”‚   3. Run full ETL sync                                             â”‚
â”‚      curl -X POST /api/etl/sync -d '{"batchSize": 500}'           â”‚
â”‚                                                                      â”‚
â”‚   4. Validate sync                                                  â”‚
â”‚      curl /api/etl/validate                                        â”‚
â”‚                                                                      â”‚
â”‚   5. Restart scheduler                                              â”‚
â”‚      curl -X POST /api/etl/scheduler/start                         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scenario 2: ETL Service Crashes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ETL CRASH RECOVERY                                â”‚
â”‚                                                                      â”‚
â”‚   Problem: ETL pod crashed during sync                              â”‚
â”‚                                                                      â”‚
â”‚   Automatic Recovery:                                               â”‚
â”‚   â”œâ”€â”€ Kubernetes detects pod failure                               â”‚
â”‚   â”œâ”€â”€ Restarts pod automatically                                   â”‚
â”‚   â”œâ”€â”€ Pod reconnects to databases                                  â”‚
â”‚   â””â”€â”€ Scheduler resumes at next interval                           â”‚
â”‚                                                                      â”‚
â”‚   Manual Steps (if needed):                                         â”‚
â”‚   1. Check pod logs                                                 â”‚
â”‚      kubectl logs -l app=etl --previous                            â”‚
â”‚                                                                      â”‚
â”‚   2. Verify databases are healthy                                   â”‚
â”‚      curl /api/etl/health                                          â”‚
â”‚                                                                      â”‚
â”‚   3. Run validation                                                 â”‚
â”‚      curl /api/etl/validate                                        â”‚
â”‚                                                                      â”‚
â”‚   4. Trigger manual sync if needed                                  â”‚
â”‚      curl -X POST /api/etl/sync                                    â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scenario 3: Network Partition

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 NETWORK PARTITION HANDLING                           â”‚
â”‚                                                                      â”‚
â”‚   Problem: ETL can't reach one of the databases                    â”‚
â”‚                                                                      â”‚
â”‚   Built-in Protection:                                              â”‚
â”‚   â”œâ”€â”€ Connection timeout prevents hanging                          â”‚
â”‚   â”œâ”€â”€ Errors are logged                                            â”‚
â”‚   â”œâ”€â”€ Sync fails gracefully                                        â”‚
â”‚   â””â”€â”€ Next scheduled run will retry                                â”‚
â”‚                                                                      â”‚
â”‚   Monitoring:                                                       â”‚
â”‚   â”œâ”€â”€ Health check shows which DB is unreachable                   â”‚
â”‚   â”œâ”€â”€ Alert on consecutive sync failures                           â”‚
â”‚   â””â”€â”€ Dashboard shows sync status                                  â”‚
â”‚                                                                      â”‚
â”‚   Response:                                                         â”‚
â”‚   â”œâ”€â”€ Investigate network issue                                    â”‚
â”‚   â”œâ”€â”€ Check database pod status                                    â”‚
â”‚   â”œâ”€â”€ Verify network policies                                      â”‚
â”‚   â””â”€â”€ Sync will auto-recover when network restores                 â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Operational Runbook

### Daily Operations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DAILY CHECKLIST                                   â”‚
â”‚                                                                      â”‚
â”‚   Morning Check:                                                    â”‚
â”‚   â˜ Verify ETL pods are running                                   â”‚
â”‚      kubectl get pods -l app=etl                                   â”‚
â”‚                                                                      â”‚
â”‚   â˜ Check scheduler status                                         â”‚
â”‚      curl /api/etl/scheduler/status                                â”‚
â”‚                                                                      â”‚
â”‚   â˜ Review sync stats                                              â”‚
â”‚      curl /api/etl/stats                                           â”‚
â”‚                                                                      â”‚
â”‚   â˜ Check for sync errors in logs                                  â”‚
â”‚      kubectl logs -l app=etl --since=24h | grep -i error          â”‚
â”‚                                                                      â”‚
â”‚   â˜ Validate data consistency                                      â”‚
â”‚      curl /api/etl/validate                                        â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Weekly Maintenance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WEEKLY MAINTENANCE                                â”‚
â”‚                                                                      â”‚
â”‚   â˜ Review sync metrics trends                                     â”‚
â”‚      - Is duration increasing? (May need optimization)             â”‚
â”‚      - Is error rate increasing? (May have data issues)            â”‚
â”‚                                                                      â”‚
â”‚   â˜ Check memory usage trends                                      â”‚
â”‚      - Memory leak detection                                       â”‚
â”‚                                                                      â”‚
â”‚   â˜ Validate backup procedures work                                â”‚
â”‚                                                                      â”‚
â”‚   â˜ Review and rotate logs                                         â”‚
â”‚                                                                      â”‚
â”‚   â˜ Update dependencies if security patches available             â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Emergency Procedures

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 EMERGENCY: STOP ALL SYNCS                            â”‚
â”‚                                                                      â”‚
â”‚   Reason: Discovered data corruption, need to investigate          â”‚
â”‚                                                                      â”‚
â”‚   Steps:                                                            â”‚
â”‚   1. Disable scheduler                                              â”‚
â”‚      curl -X POST /api/etl/scheduler/stop                          â”‚
â”‚                                                                      â”‚
â”‚   2. Scale down to 0 pods (prevents any sync)                      â”‚
â”‚      kubectl scale deployment etl-depl --replicas=0               â”‚
â”‚                                                                      â”‚
â”‚   3. Investigate the issue                                          â”‚
â”‚                                                                      â”‚
â”‚   4. Fix the issue                                                  â”‚
â”‚                                                                      â”‚
â”‚   5. Scale back up                                                  â”‚
â”‚      kubectl scale deployment etl-depl --replicas=1               â”‚
â”‚                                                                      â”‚
â”‚   6. Run dry-run sync first                                        â”‚
â”‚      curl -X POST /api/etl/sync -d '{"dryRun": true}'             â”‚
â”‚                                                                      â”‚
â”‚   7. If safe, run actual sync                                      â”‚
â”‚      curl -X POST /api/etl/sync                                    â”‚
â”‚                                                                      â”‚
â”‚   8. Re-enable scheduler                                            â”‚
â”‚      curl -X POST /api/etl/scheduler/start                         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Performance Tuning

### Database Optimization

```sql
-- PostgreSQL: Add indexes for ETL queries
CREATE INDEX idx_product_id ON product(id);
CREATE INDEX idx_cart_user_id ON cart(userId);
CREATE INDEX idx_cart_product_id ON cart(productId);

-- Analyze tables for query planner
ANALYZE product;
ANALYZE cart;
```

### Connection Pool Sizing

```typescript
// TypeORM connection pool
const dataSource = new DataSource({
    type: 'postgres',
    url: process.env.CART_DB_URL,
    
    // Connection pool settings
    extra: {
        max: 10,              // Maximum connections
        min: 2,               // Minimum connections
        idleTimeoutMillis: 30000,  // Close idle connections after 30s
    },
});
```

### Batch Size Tuning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BATCH SIZE GUIDELINES                             â”‚
â”‚                                                                      â”‚
â”‚   Data Volume    â”‚ Recommended Batch â”‚ Delay Between â”‚              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚   < 1,000        â”‚ 100               â”‚ 50ms          â”‚              â”‚
â”‚   1K - 10K       â”‚ 200               â”‚ 100ms         â”‚              â”‚
â”‚   10K - 100K     â”‚ 500               â”‚ 100ms         â”‚              â”‚
â”‚   > 100K         â”‚ 1000              â”‚ 200ms         â”‚              â”‚
â”‚                                                                      â”‚
â”‚   Tune based on:                                                    â”‚
â”‚   â”œâ”€â”€ Available memory                                             â”‚
â”‚   â”œâ”€â”€ Database connection limits                                   â”‚
â”‚   â”œâ”€â”€ Network bandwidth                                            â”‚
â”‚   â””â”€â”€ Acceptable sync duration                                     â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Continuous Improvement

### Metrics to Track Over Time

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 IMPROVEMENT METRICS                                  â”‚
â”‚                                                                      â”‚
â”‚   Track these weekly/monthly:                                       â”‚
â”‚                                                                      â”‚
â”‚   Performance:                                                      â”‚
â”‚   â”œâ”€â”€ Average sync duration                                        â”‚
â”‚   â”œâ”€â”€ P95/P99 sync duration                                        â”‚
â”‚   â”œâ”€â”€ Records synced per second                                    â”‚
â”‚   â””â”€â”€ Database query times                                         â”‚
â”‚                                                                      â”‚
â”‚   Reliability:                                                      â”‚
â”‚   â”œâ”€â”€ Sync success rate                                            â”‚
â”‚   â”œâ”€â”€ Error frequency by type                                      â”‚
â”‚   â”œâ”€â”€ Time between failures                                        â”‚
â”‚   â””â”€â”€ Recovery time                                                â”‚
â”‚                                                                      â”‚
â”‚   Data Quality:                                                     â”‚
â”‚   â”œâ”€â”€ Records out of sync before each run                         â”‚
â”‚   â”œâ”€â”€ Data validation errors                                       â”‚
â”‚   â””â”€â”€ Orphaned records                                             â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Key Takeaways

| Area | Best Practice |
|------|---------------|
| **Deployment** | Use rolling updates, blue-green for major changes |
| **Kubernetes** | Set resource limits, health probes, PDBs |
| **Security** | Secrets in K8s, least privilege, network policies |
| **Disaster Recovery** | Document procedures, practice recovery |
| **Operations** | Daily health checks, weekly maintenance |
| **Performance** | Tune batch size, indexes, connection pools |

---

## â¡ï¸ Next Chapter
[Chapter 8: Quick Reference & Glossary](./08-quick-reference.md)

